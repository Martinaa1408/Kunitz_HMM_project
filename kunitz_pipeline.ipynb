{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Kunitz Domain Detection Using Profile Hidden Markov Models (HMM)\n",
    "\n",
    "\n",
    "##### **Author**: **Martina Castellucci**\n",
    "\n",
    "This notebook implements a bioinformatics pipeline to identify the Kunitz-type protease inhibitor domain (PF00014) in protein sequences using HMM-based detection.\n",
    "\n",
    "We will:\n",
    "- Collect and preprocess positive and negative datasets.\n",
    "- Reduce redundancy using CD-HIT and extract structural alignments from PDB.\n",
    "- Build an HMM profile using `hmmbuild`.\n",
    "- Filter sequences to avoid bias in evaluation.\n",
    "- Run `hmmsearch` on test sets.\n",
    "- Evaluate model performance with multiple thresholds.\n",
    "\n",
    "## Linux Command-Line Tools: Summary of Usage\n",
    "\n",
    "This section summarizes the most commonly used Linux shell commands in this notebook:\n",
    "\n",
    "- **`cat`**: Concatenates and displays the content of files.\n",
    "- **`awk`**: A powerful text-processing tool used for pattern scanning and field-based extraction.\n",
    "- **`grep`**: Searches for lines matching a pattern within a file.\n",
    "- **`cut`**: Extracts specific columns or fields from each line of a file.\n",
    "- **`sort`**: Sorts lines in a file, alphabetically or numerically.\n",
    "- **`comm`**: Compares two sorted files and shows common and distinct lines.\n",
    "- **`makeblastdb`**: Creates a searchable BLAST database from a FASTA file.\n",
    "- **`blastp`**: Compares protein sequences using BLAST against a protein database.\n",
    "- **`cd-hit`**: Clusters sequences to reduce redundancy based on sequence identity thresholds.\n",
    "- **`hmmbuild`**: Builds a profile Hidden Markov Model (HMM) from a multiple sequence alignment.\n",
    "- **`hmmsearch`**: Searches sequence databases for matches to a profile HMM.\n",
    "- **`head` / `tail`**: Outputs the first or last N lines of a file (useful for dataset splitting).\n",
    "- **`wc`**: Word count; used with `-l` to count lines (e.g., number of sequences).\n",
    "- **`less`**: Paginates the display of long files (useful for inspection).\n",
    "- **`clstr2txt.pl`**: Converts .clstr output from CD-HIT into a readable tabular format that lists each     sequence and identifies the representative of each cluster."
   ],
   "id": "a375b3be44f29249"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Extract Human and Non-Human Kunitz Sequences\n",
    "\n",
    "From the file `all_kunitz.fasta`, extract sequences annotated as \"Homo sapiens\" and separate them from non-human sequences."
   ],
   "id": "4009074662b605ae"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract human sequences\n",
    "awk '/^>/ {f=($0 ~ /Homo sapiens/)} f' all_kunitz.fasta > human_kunitz.fasta\n",
    "\n",
    "# Extract non-human sequences\n",
    "grep -v \"Homo sapiens\" all_kunitz.fasta > nothuman_kunitz.fasta\n",
    "\n",
    "# Count the number of human sequences (headers only)\n",
    "grep \"sp\" human_kunitz.fasta | wc -l"
   ],
   "id": "3d71cf243b0d840b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Extract PF00014 sequences from PDB report and cluster with CD-HIT\n",
    "\n",
    "Download a CSV from RCSB PDB filtered for:\n",
    "- PFAM = PF00014\n",
    "- Length 45–80 aa\n",
    "- Resolution ≤ 3.5 Å\n",
    "\n",
    "Then extract the sequences in FASTA and reduce redundancy using CD-HIT at 90%.\n"
   ],
   "id": "13f712670aa6fcc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract PF00014 sequences from the CSV\n",
    "cat rcsb_pdb_custom_report_20250410062557.csv | tr -d '\"' \\\n",
    "| awk -F ',' '{if (length($2)>0) {name=$2}; print name ,$3,$4,$5}' \\\n",
    "| grep PF00014 \\\n",
    "| awk '{print \">\"$1\"_\"$3; print $2}' > pdb_kunitz_customreported.fasta\n",
    "\n",
    "# Remove redundancy with CD-HIT (90% sequence identity)\n",
    "cd-hit -i pdb_kunitz_customreported.fasta -o pdb_kunitz_customreported_nr.fasta -c 0.9"
   ],
   "id": "72a10a8a98788693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Extraction of representative sequences after CD-HIT\n",
    "\n",
    "After running CD-HIT, we convert the `.clstr` file into a readable format, extract the representative IDs for each cluster, and retrieve the corresponding FASTA sequences. This allows us to work with a non-redundant dataset.\n"
   ],
   "id": "b92a41c8ccf7f726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert the .clstr file generated by CD-HIT into a readable format\n",
    "clstr2txt.pl pdb_kunitz_customreported_filtered.clstr > pdb_kunitz.clusters.txt\n",
    "\n",
    "# Extract representative sequence IDs (those with value 1 in the fifth column)\n",
    "awk '$5 == 1 {print $1}' pdb_kunitz.clusters.txt > pdb_kunitz_rp.ids\n",
    "\n",
    "# Extract FASTA sequences corresponding to representative IDs\n",
    "> pdb_kunitz_rp.fasta\n",
    "for i in $(cat pdb_kunitz_rp.ids); do\n",
    "    grep -A 1 \"^>$i\" pdb_kunitz_customreported.fasta | head -n 2 >> pdb_kunitz_rp.fasta\n",
    "done\n",
    "\n",
    "# Prepare the ID list for PDBefold by converting \"_\" to \":\" as required\n",
    "grep \">\" pdb_kunitz_rp.fasta | tr -d \">\" | tr \"_\" \":\" > tmp_pdb_efold_ids.txt"
   ],
   "id": "f5e5623b8028b37e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Structural Alignment and HMM Building\n",
    "\n",
    "Use the representative sequences to submit to PDBeFold and download the `.ali` alignment. Reformat for `hmmbuild`, then build the structural HMM.\n"
   ],
   "id": "2fe533854c6a2c9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reformat the .ali file into a FASTA-like format\n",
    "awk '{\n",
    "  if (substr($1,1,1)==\">\") {\n",
    "    print \"\\n\" toupper($1)\n",
    "  } else {\n",
    "    printf \"%s\", toupper($1)\n",
    "  }\n",
    "}' pdb_kunitz_rp.ali > pdb_kunitz_rp_formatted.ali\n",
    "\n",
    "# Build the HMM model from the reformatted alignment\n",
    "hmmbuild structural_model.hmm pdb_kunitz_rp_formatted.ali"
   ],
   "id": "410a3d59b2a7065"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. BLAST Filtering to Remove Overlapping Sequences\n",
    "\n",
    "To avoid evaluating the same sequences used to build the model, we perform BLAST between the representative PDB Kunitz sequences and the full UniProt Kunitz set.\n",
    "\n",
    "We retain only sequences with identity < 95% and coverage < 50%.\n"
   ],
   "id": "b43fd2a525b3ca13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create the BLAST database from all Kunitz sequences (human + non-human)\n",
    "makeblastdb -in all_kunitz.fasta -dbtype prot -out all_kunitz.fasta\n",
    "\n",
    "# Run BLAST\n",
    "blastp -query pdb_kunitz_rp.fasta -db all_kunitz.fasta -out pdb_kunitz_nr_23.blast -outfmt 7"
   ],
   "id": "ed7eb193800a9ff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract UniProt IDs to exclude (identity ≥ 95% and coverage ≥ 50%)\n",
    "grep -v \"^#\" pdb_kunitz_nr_23.blast | awk '{if ($3>=95 && $4>=50) print $2}' | sort -u | cut -d \"|\" -f 2 > to_remove.ids\n",
    "\n",
    "# Extract all UniProt IDs from Kunitz sequences\n",
    "grep \">\" all_kunitz.fasta | cut -d \"|\" -f 2 > all_kunitz.id\n",
    "\n",
    "# Get the list of IDs to keep (not too similar to PDB set)\n",
    "comm -23 <(sort all_kunitz.id) <(sort to_remove.ids) > to_keep.ids"
   ],
   "id": "ff663a2ab1bfd6eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract the final sequences to keep (valid positives for testing)\n",
    "python3 get_seq.py to_keep.ids all_kunitz.fasta ok_kunitz.fasta"
   ],
   "id": "4e1feba3267d30dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "89a3fb92d93d454b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Construction of the Negative Dataset\n",
    "\n",
    "Extract from SwissProt all proteins that do NOT contain the Kunitz domain and create the negative FASTA dataset.\n"
   ],
   "id": "b09a1011484fc0a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get all UniProt IDs from SwissProt\n",
    "grep \">\" uniprot_sprot.fasta | cut -d \"|\" -f 2 > sp.id\n",
    "\n",
    "# Exclude IDs that belong to Kunitz sequences\n",
    "comm -23 <(sort sp.id) <(sort all_kunitz.id) > sp_negs.ids\n",
    "\n",
    "# Extract the final negative sequences\n",
    "python3 get_seq.py sp_negs.ids uniprot_sprot.fasta sp_negs.fasta"
   ],
   "id": "39aadc20fcf66053"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "40fe1a66db9c391d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Train/Test Set Construction\n",
    "\n",
    "Split the positive and negative datasets into training and testing halves using randomization.\n"
   ],
   "id": "7c21d52e94c3a1e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Randomize the IDs\n",
    "sort -R to_keep.ids > random_ok_kunitz.ids\n",
    "sort -R sp_negs.ids > random_sp_negs.ids\n",
    "\n",
    "# Split into two halves\n",
    "head -n 183 random_ok_kunitz.ids > pos_1.ids\n",
    "tail -n 183 random_ok_kunitz.ids > pos_2.ids\n",
    "\n",
    "head -n 286417 random_sp_negs.ids > neg_1.ids\n",
    "tail -n 286417 random_sp_negs.ids > neg_2.ids"
   ],
   "id": "b969ed1425fe62d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract FASTA sequences for the 4 datasets\n",
    "python3 get_seq.py pos_1.ids uniprot_sprot.fasta > pos_1.fasta\n",
    "python3 get_seq.py pos_2.ids uniprot_sprot.fasta > pos_2.fasta\n",
    "python3 get_seq.py neg_1.ids uniprot_sprot.fasta > neg_1.fasta\n",
    "python3 get_seq.py neg_2.ids uniprot_sprot.fasta > neg_2.fasta"
   ],
   "id": "4ee19936828ff7ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. HMMER Search and .class File Generation\n",
    "\n",
    "We run `hmmsearch` on each of the four FASTA sets (pos_1, pos_2, neg_1, neg_2) using the trained HMM.  \n",
    "To ensure consistency in e-value calculation across datasets of different sizes, we use the `-Z 1000` option.  \n",
    "We convert the output to `.class` format to later evaluate classification performance.\n"
   ],
   "id": "fd467523e0036724"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run hmmsearch with tabular output for each dataset\n",
    "hmmsearch -Z 1000 --max --tblout pos_1.out structural_model.hmm pos_1.fasta\n",
    "hmmsearch -Z 1000 --max --tblout pos_2.out structural_model.hmm pos_2.fasta\n",
    "hmmsearch -Z 1000 --max --tblout neg_1.out structural_model.hmm neg_1.fasta\n",
    "hmmsearch -Z 1000 --max --tblout neg_2.out structural_model.hmm neg_2.fasta"
   ],
   "id": "16f2d24c624a7670"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We now extract useful information from the HMMER output:\n",
    "- Identifier\n",
    "- True label (1 for positive, 0 for negative)\n",
    "- Full-sequence E-value\n",
    "- Domain E-value (if available)\n",
    "\n",
    "These are stored in `.class` files to be used with the `performance.py` script.\n"
   ],
   "id": "772b612d5aa96c08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# POSITIVES\n",
    "grep -v \"^#\" pos_1.out | awk '{split($1,a,\"|\"); print a[2]\"\\t1\\t\"$5\"\\t\"$8}' > pos_1.class\n",
    "grep -v \"^#\" pos_2.out | awk '{split($1,a,\"|\"); print a[2]\"\\t1\\t\"$5\"\\t\"$8}' > pos_2.class\n",
    "\n",
    "# NEGATIVES\n",
    "grep -v \"^#\" neg_1.out | awk '{split($1,a,\"|\"); print a[2]\"\\t0\\t\"$5\"\\t\"$8}' > neg_1.class\n",
    "grep -v \"^#\" neg_2.out | awk '{split($1,a,\"|\"); print a[2]\"\\t0\\t\"$5\"\\t\"$8}' > neg_2.class\n"
   ],
   "id": "c5fd8bc1ca8a08c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If `hmmsearch` does not report a match, that sequence is still part of the dataset and should be considered a true negative.  \n",
    "We assign them a default E-value of 10.0 and append them to the `.class` files using `comm`.\n"
   ],
   "id": "6b64e13b4c63795a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# UNMATCHED NEGATIVES: add them manually with E-value 10.0\n",
    "comm -23 <(sort neg_1.ids) <(cut -f1 neg_1.class | sort) | awk '{print $1\"\\t0\\t10.0\\t10.0\"}' >> neg_1_hits.class\n",
    "comm -23 <(sort neg_2.ids) <(cut -f1 neg_2.class | sort) | awk '{print $1\"\\t0\\t10.0\\t10.0\"}' >> neg_2_hits.class"
   ],
   "id": "97203b83152c73ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Merge Datasets\n",
    "\n",
    "We now merge the positive and negative `.class` files into two evaluation sets (`set_1` and `set_2`).\n"
   ],
   "id": "920f4083838ec36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge the datasets\n",
    "cat pos_1.class neg_1_hits.class > set_1.class\n",
    "cat pos_2.class neg_2_hits.class > set_2.class\n"
   ],
   "id": "fd8d106262a65bc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. Evaluate Model Performance at Different Thresholds and Analyze Errors\n",
    "\n",
    "In this step, we evaluate model performance at different E-value thresholds using `performance.py`. The goal is to identify the optimal threshold based on the **Matthews Correlation Coefficient (MCC)**.\n",
    "\n",
    "We also analyze false negatives—true positive sequences with high E-values that were misclassified as negatives. This helps us identify potential sensitivity issues in the model.\n",
    "\n",
    "- Run performance evaluation for both `set_1.class` and `set_2.class` at a fixed threshold (`1e-5`)\n",
    "- Repeat evaluation across multiple thresholds (from `1e-1` to `1e-10`)\n",
    "- Sort results based on MCC to find the best cutoff\n",
    "- Extract and save the worst false negatives for comparison and further inspection\n"
   ],
   "id": "e132eb1ee99b7ebb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute performance at a fixed threshold\n",
    "python3 performance.py set_1.class 1e-5\n",
    "python3 performance.py set_2.class 1e-5\n",
    "\n",
    "# Repeat for different E-value thresholds to find the optimal one (based on MCC)\n",
    "for i in $(seq 1 10); do\n",
    "  python3 performance.py set_1.class 1e-$i\n",
    "done | sort -nrk 6  # Sort by the sixth column (MCC)\n",
    "\n",
    "# Error analysis – Identify false negatives with high E-values\n",
    "sort -grk 3 pos_1.class | less\n",
    "\n",
    "# Extract the worst false negatives into separate files for further inspection\n",
    "awk '$2 == 1 && $3 > 1e-5' pos_1.class | sort -grk 3 > fn_pos1.txt\n",
    "awk '$2 == 1 && $3 > 1e-5' pos_2.class | sort -grk 3 > fn_pos2.txt"
   ],
   "id": "22400757f6562c25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "41d6af6d559ca8c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "be83807ae844c181"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
